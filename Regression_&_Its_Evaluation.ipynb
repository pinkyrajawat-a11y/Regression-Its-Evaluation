{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EMvGhLGRBOGs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b4ADFyaLBNll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ztkbx4W9Sbp6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qDj1elVISdeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XfHdHq-pSeYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "  \n",
        "  . Simple Linear Regression is a statistical and machine learning technique used to model the relationship between one independent variable (X) and one dependent variable (Y) by fitting a straight line to the data.\n",
        "Definition\n",
        "\n",
        "It assumes a linear relationship between X and Y and represents it using the equation:\n",
        "\n",
        "ð‘Œ\n",
        "=\n",
        "ð›½\n",
        "0\n",
        "+\n",
        "ð›½\n",
        "1\n",
        "ð‘‹\n",
        "+\n",
        "ðœ€\n",
        "Y=Î²\n",
        "0\n",
        "\tâ€‹\n",
        "\n",
        "+Î²\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "X+Îµ\n",
        "ðŸ“˜ Explanation of Terms\n",
        "\n",
        "Y â†’ Dependent variable (target/output)\n",
        "\n",
        "X â†’ Independent variable (input/predictor)\n",
        "\n",
        "Î²â‚€ (Intercept) â†’ Value of Y when X = 0\n",
        "\n",
        "Î²â‚ (Slope) â†’ Change in Y for a one-unit change in X\n",
        "\n",
        "Îµ (Error term) â†’ Difference between actual and predicted values\n",
        "\n",
        "ðŸ“ˆ Example\n",
        "\n",
        "Suppose we want to predict salary (Y) based on years of experience (X):\n",
        "\n",
        "As experience increases, salary tends to increase.\n",
        "\n",
        "Simple linear regression finds the best straight line that predicts salary from experience.\n",
        "\n",
        "ðŸŽ¯ Objective\n",
        "\n",
        "The goal is to find the values of Î²â‚€ and Î²â‚ that minimize the sum of squared errors between actual and predicted values (this is done using the Least Squares Method).\n",
        "\n",
        "âœ… Assumptions\n",
        "\n",
        "Linear relationship between X and Y\n",
        "\n",
        "Independence of observations\n",
        "\n",
        "Homoscedasticity (constant variance of errors)\n",
        "\n",
        "Normally distributed errors\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        " . The key assumptions of Simple Linear Regression ensure that the model gives reliable and valid results. There are four main assumptions:\n",
        "\n",
        "1ï¸âƒ£ Linearity\n",
        "\n",
        "The relationship between the independent variable (X) and dependent variable (Y) is linear.\n",
        "\n",
        "This means changes in Y are proportional to changes in X.\n",
        "\n",
        "ðŸ“Š Can be checked using a scatter plot.\n",
        "\n",
        "2ï¸âƒ£ Independence of Errors\n",
        "\n",
        "The residuals (errors) are independent of each other.\n",
        "\n",
        "One observation should not influence another.\n",
        "\n",
        "Commonly violated in time series data.\n",
        "\n",
        "3ï¸âƒ£ Homoscedasticity\n",
        "\n",
        "The variance of residuals is constant across all levels of X.\n",
        "\n",
        "If variance changes, it is called heteroscedasticity.\n",
        "\n",
        "ðŸ“‰ Checked using a residuals vs. fitted values plot.\n",
        "\n",
        "4ï¸âƒ£ Normality of Errors\n",
        "\n",
        "The residuals should be normally distributed.\n",
        "\n",
        "Important for valid confidence intervals and hypothesis testing.\n",
        "\n",
        "ðŸ“ˆ Checked using histograms or Qâ€“Q plots.\n",
        "\n",
        "ðŸ“Œ Summary Table\n",
        "Assumption\tMeaning\n",
        "Linearity\tStraight-line relationship between X and Y\n",
        "Independence\tErrors are not correlated\n",
        "Homoscedasticity\tConstant error variance\n",
        "Normality\tErrors follow normal distribution\n",
        "\n",
        "If you want, I can also:\n",
        "\n",
        "Explain how to test these assumptions in Python\n",
        "\n",
        "Show real-world examples\n",
        "\n",
        "Compare with assumptions of multiple linear regression\n",
        "\n",
        "3. What is heteroscedasticity, and why is it important to address in regression\n",
        "models?\n",
        "    .  Heteroscedasticity occurs in a regression model when the variance of the error terms (residuals) is not constant across all levels of the independent variable(s).\n",
        "\n",
        "In simple terms, as the value of X changes, the spread of the residuals increases or decreases instead of remaining uniform.\n",
        "\n",
        "ðŸ“‰ Visual sign:\n",
        "In a residuals vs. fitted values plot, heteroscedasticity often appears as a fan-shaped or cone-shaped pattern.\n",
        "\n",
        "ðŸ“Š Example\n",
        "\n",
        "Consider predicting income based on work experience:\n",
        "\n",
        "For low experience, incomes are clustered closely.\n",
        "\n",
        "For high experience, incomes vary widely.\n",
        "âž¡ï¸ This increasing variance indicates heteroscedasticity.\n",
        "\n",
        "â— Why Is Heteroscedasticity a Problem?\n",
        "\n",
        "While regression coefficients may still be unbiased, heteroscedasticity causes several serious issues:\n",
        "\n",
        "1ï¸âƒ£ Invalid standard errors\n",
        "\n",
        "Leads to incorrect t-tests and F-tests\n",
        "\n",
        "2ï¸âƒ£ Misleading confidence intervals\n",
        "\n",
        "Intervals may be too wide or too narrow\n",
        "\n",
        "3ï¸âƒ£ Unreliable hypothesis testing\n",
        "\n",
        "Increases risk of Type I and Type II errors\n",
        "\n",
        "4ï¸âƒ£ Inefficient estimates\n",
        "\n",
        "The model does not make the best use of data\n",
        "\n",
        "ðŸ” How to Detect Heteroscedasticity\n",
        "\n",
        "Residuals vs. fitted values plot\n",
        "\n",
        "Breuschâ€“Pagan test\n",
        "\n",
        "White test\n",
        "\n",
        "ðŸ› ï¸ How to Address Heteroscedasticity\n",
        "\n",
        "âœ”ï¸ Transform variables (log, square root)\n",
        "âœ”ï¸ Use heteroscedasticity-robust standard errors\n",
        "âœ”ï¸ Weighted Least Squares (WLS)\n",
        "âœ”ï¸ Add missing variables to the model\n",
        "\n",
        "4.  What is Multiple Linear Regression?\n",
        "\n",
        "      .  Multiple Linear Regression (MLR) is a statistical and machine learning technique used to model the relationship between one dependent variable (Y) and two or more independent variables (Xâ‚, Xâ‚‚, â€¦, Xâ‚™).\n",
        "\n",
        "It extends Simple Linear Regression by allowing multiple predictors to explain the outcome.\n",
        "\n",
        "ðŸ§® Mathematical Equation\n",
        "ð‘Œ\n",
        "=\n",
        "ð›½\n",
        "0\n",
        "+\n",
        "ð›½\n",
        "1\n",
        "ð‘‹\n",
        "1\n",
        "+\n",
        "ð›½\n",
        "2\n",
        "ð‘‹\n",
        "2\n",
        "+\n",
        "â‹¯\n",
        "+\n",
        "ð›½\n",
        "ð‘›\n",
        "ð‘‹\n",
        "ð‘›\n",
        "+\n",
        "ðœ€\n",
        "Y=Î²\n",
        "0\n",
        "\tâ€‹\n",
        "\n",
        "+Î²\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "+Î²\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "+â‹¯+Î²\n",
        "n\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "n\n",
        "\tâ€‹\n",
        "\n",
        "+Îµ\n",
        "ðŸ“˜ Explanation of Terms\n",
        "\n",
        "Y â†’ Dependent variable (target/output)\n",
        "\n",
        "Xâ‚, Xâ‚‚, â€¦, Xâ‚™ â†’ Independent variables (predictors)\n",
        "\n",
        "Î²â‚€ (Intercept) â†’ Value of Y when all Xâ€™s are 0\n",
        "\n",
        "Î²â‚, Î²â‚‚, â€¦, Î²â‚™ (Coefficients) â†’ Effect of each X on Y (holding others constant)\n",
        "\n",
        "Îµ (Error term) â†’ Random error\n",
        "\n",
        "ðŸ“Š Example\n",
        "\n",
        "Predicting house price (Y) using:\n",
        "\n",
        "House size (Xâ‚)\n",
        "\n",
        "Number of bedrooms (Xâ‚‚)\n",
        "\n",
        "Location score (Xâ‚ƒ)\n",
        "\n",
        "Each coefficient shows how much the price changes when that variable changes, keeping other variables constant.\n",
        "\n",
        "ðŸŽ¯ Objective\n",
        "\n",
        "The goal is to estimate coefficients that minimize the sum of squared residuals, typically using the Least Squares Method.\n",
        "\n",
        "5. What is polynomial regression, and how does it differ from linear\n",
        "regression?\n",
        "\n",
        "   . What is Polynomial Regression?\n",
        "\n",
        "Polynomial regression is a regression technique used to model a non-linear relationship between the independent variable(s) and the dependent variable by including polynomial terms (such as\n",
        "ð‘‹\n",
        "2\n",
        ",\n",
        "ð‘‹\n",
        "3\n",
        ",\n",
        "X\n",
        "2\n",
        ",X\n",
        "3\n",
        ", etc.) in the model.\n",
        "\n",
        "Even though it fits a curve, it is still considered a type of linear regression because it is linear in its parameters (coefficients).\n",
        "\n",
        "ðŸ§® Mathematical Form\n",
        "\n",
        "Linear Regression\n",
        "\n",
        "ð‘Œ\n",
        "=\n",
        "ð›½\n",
        "0\n",
        "+\n",
        "ð›½\n",
        "1\n",
        "ð‘‹\n",
        "+\n",
        "ðœ€\n",
        "Y=Î²\n",
        "0\n",
        "\tâ€‹\n",
        "\n",
        "+Î²\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "X+Îµ\n",
        "\n",
        "Polynomial Regression (degree 2 example)\n",
        "\n",
        "ð‘Œ\n",
        "=\n",
        "ð›½\n",
        "0\n",
        "+\n",
        "ð›½\n",
        "1\n",
        "ð‘‹\n",
        "+\n",
        "ð›½\n",
        "2\n",
        "ð‘‹\n",
        "2\n",
        "+\n",
        "ðœ€\n",
        "Y=Î²\n",
        "0\n",
        "\tâ€‹\n",
        "\n",
        "+Î²\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "X+Î²\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "2\n",
        "+Îµ\n",
        "ðŸ” How It Differs from Linear Regression\n",
        "Aspect\tLinear Regression\tPolynomial Regression\n",
        "Relationship\tStraight-line\tCurved\n",
        "Model form\tUses only\n",
        "ð‘‹\n",
        "X\tUses\n",
        "ð‘‹\n",
        ",\n",
        "ð‘‹\n",
        "2\n",
        ",\n",
        "ð‘‹\n",
        "3\n",
        ",\n",
        "â€¦\n",
        "X,X\n",
        "2\n",
        ",X\n",
        "3\n",
        ",â€¦\n",
        "Ability to fit non-linear data\tLimited\tBetter\n",
        "Complexity\tSimple\tMore complex\n",
        "Risk of overfitting\tLow\tHigher for high-degree polynomials\n",
        "ðŸ“Š Example\n",
        "\n",
        "Linear regression assumes the dependent variable changes at a constant rate with X.\n",
        "\n",
        "Polynomial regression can model situations where the rate of change itself varies (e.g., accelerating or decelerating growth).\n",
        "\n",
        "âš ï¸ Key Consideration\n",
        "\n",
        "Choosing a very high polynomial degree may lead to overfitting, where the model fits noise instead of the true pattern.\n",
        "\n",
        "âœ… Summary\n",
        "\n",
        "Linear regression fits a straight line\n",
        "\n",
        "Polynomial regression fits a curve\n",
        "\n",
        "Polynomial regression is useful when data shows a non-linear trend\n",
        "  \n",
        "\n",
        "  6.\n",
        "\n"
      ],
      "metadata": {
        "id": "qd2pdxT91Qj6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VZ3NwJBSSpPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-1gn91HXSxGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "znacT29DSyMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "6. Implement a Python program to fit a Simple Linear Regression model to\n",
        "the following sample data:\n",
        "â— X = [1, 2, 3, 4, 5]\n",
        "â— Y = [2.1, 4.3, 6.1, 7.9, 10.2]\n",
        "Plot the regression line over the data points\n",
        "\n",
        ".  Plot the regression line over the data points\n",
        "\n",
        "Iâ€™ve implemented a Simple Linear Regression model in Python using your given data and plotted the regression line over the data points.\n",
        "\n",
        "ðŸ” Model Result\n",
        "\n",
        "From the fitted model:\n",
        "\n",
        "Intercept (Î²â‚€) â‰ˆ 0.18\n",
        "\n",
        "Slope (Î²â‚) â‰ˆ 1.98\n",
        "\n",
        "So, the regression equation is approximately:\n",
        "\n",
        "ð‘Œ\n",
        "=\n",
        "0.18\n",
        "+\n",
        "1.98\n",
        "ð‘‹\n",
        "Y=0.18+1.98X\n",
        "ðŸ“ˆ Plot Explanation\n",
        "\n",
        "The dots represent the original data points.\n",
        "\n",
        "The line represents the best-fit regression line predicted by the model.\n",
        "\n",
        "The line closely follows the points, indicating a strong linear relationship between X and Y.\n",
        "\n",
        "If you want, I can also:\n",
        "\n",
        "Implement this without scikit-learn (using formulas)\n",
        "\n",
        "Calculate RÂ² and error metrics\n",
        "\n",
        "Explain this program line by line (exam-ready)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "044f4906-524f-4b83-d586-25f822980334",
        "id": "6D_yHxb3BJ3K"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid character 'â—' (U+25CF) (ipython-input-1023388473.py, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1023388473.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    â— X = [1, 2, 3, 4, 5]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character 'â—' (U+25CF)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "   8. Implement polynomial regression on the following data:\n",
        "â— X = [1, 2, 3, 4, 5]\n",
        "3\n",
        "â— Y = [2.2, 4.8, 7.5, 11.2, 14.7]\n",
        "Fit a 2nd-degree polynomial and plot the resulting curve.\n",
        " . Meanwhile, here is the correct and complete Python program you can run locally to implement 2nd-degree Polynomial Regression and plot the curve.\n",
        "\n",
        "âœ… Polynomial Regression (Degree = 2) â€“ Python Code\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Given data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "Y = np.array([2.2, 4.8, 7.5, 11.2, 14.7])\n",
        "\n",
        "# Convert to polynomial features (degree 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, Y)\n",
        "\n",
        "# Predict for smooth curve\n",
        "X_range = np.linspace(1, 5, 100).reshape(-1, 1)\n",
        "X_range_poly = poly.transform(X_range)\n",
        "Y_pred = model.predict(X_range_poly)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, Y)\n",
        "plt.plot(X_range, Y_pred)\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"Polynomial Regression (Degree 2)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "HdTb7T8TTwas",
        "outputId": "cfebdbde-cb61-4897-cddf-ca98245cdff7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid character 'â—' (U+25CF) (ipython-input-2185591033.py, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2185591033.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    â— X = [1, 2, 3, 4, 5]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character 'â—' (U+25CF)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IHgVyRwxUV7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9.  Create a residuals plot for a regression model trained on this data:\n",
        "â— X = [10, 20, 30, 40, 50]\n",
        "â— Y = [15, 35, 40, 50, 65]\n",
        "Assess heteroscedasticity by examining the spread of residuals.\n",
        "\n",
        ". We are given:\n",
        "\n",
        "ð‘‹\n",
        "=\n",
        "[\n",
        "10\n",
        ",\n",
        "20\n",
        ",\n",
        "30\n",
        ",\n",
        "40\n",
        ",\n",
        "50\n",
        "]\n",
        "X=[10,20,30,40,50]\n",
        "ð‘Œ\n",
        "=\n",
        "[\n",
        "15\n",
        ",\n",
        "35\n",
        ",\n",
        "40\n",
        ",\n",
        "50\n",
        ",\n",
        "65\n",
        "]\n",
        "Y=[15,35,40,50,65]\n",
        "Step 1: Fit a Simple Linear Regression\n",
        "\n",
        "The linear regression model has the form:\n",
        "\n",
        "ð‘Œ\n",
        "=\n",
        "ð‘Ž\n",
        "ð‘‹\n",
        "+\n",
        "ð‘\n",
        "Y=aX+b\n",
        "\n",
        "We can calculate the slope\n",
        "ð‘Ž\n",
        "a and intercept\n",
        "ð‘\n",
        "b using formulas:\n",
        "\n",
        "ð‘Ž\n",
        "=\n",
        "ð‘›\n",
        "âˆ‘\n",
        "ð‘‹\n",
        "ð‘Œ\n",
        "âˆ’\n",
        "âˆ‘\n",
        "ð‘‹\n",
        "âˆ‘\n",
        "ð‘Œ\n",
        "ð‘›\n",
        "âˆ‘\n",
        "ð‘‹\n",
        "2\n",
        "âˆ’\n",
        "(\n",
        "âˆ‘\n",
        "ð‘‹\n",
        ")\n",
        "2\n",
        ",\n",
        "ð‘\n",
        "=\n",
        "âˆ‘\n",
        "ð‘Œ\n",
        "âˆ’\n",
        "ð‘Ž\n",
        "âˆ‘\n",
        "ð‘‹\n",
        "ð‘›\n",
        "a=\n",
        "nâˆ‘X\n",
        "2\n",
        "âˆ’(âˆ‘X)\n",
        "2\n",
        "nâˆ‘XYâˆ’âˆ‘Xâˆ‘Y\n",
        "\tâ€‹\n",
        "\n",
        ",b=\n",
        "n\n",
        "âˆ‘Yâˆ’aâˆ‘X\n",
        "\tâ€‹\n",
        "\n",
        "\n",
        "Compute sums:\n",
        "\n",
        "ð‘›\n",
        "=\n",
        "5\n",
        "n=5\n",
        "\n",
        "âˆ‘\n",
        "ð‘‹\n",
        "=\n",
        "10\n",
        "+\n",
        "20\n",
        "+\n",
        "30\n",
        "+\n",
        "40\n",
        "+\n",
        "50\n",
        "=\n",
        "150\n",
        "âˆ‘X=10+20+30+40+50=150\n",
        "\n",
        "âˆ‘\n",
        "ð‘Œ\n",
        "=\n",
        "15\n",
        "+\n",
        "35\n",
        "+\n",
        "40\n",
        "+\n",
        "50\n",
        "+\n",
        "65\n",
        "=\n",
        "205\n",
        "âˆ‘Y=15+35+40+50+65=205\n",
        "\n",
        "âˆ‘\n",
        "ð‘‹\n",
        "ð‘Œ\n",
        "=\n",
        "10\n",
        "âˆ—\n",
        "15\n",
        "+\n",
        "20\n",
        "âˆ—\n",
        "35\n",
        "+\n",
        "30\n",
        "âˆ—\n",
        "40\n",
        "+\n",
        "40\n",
        "âˆ—\n",
        "50\n",
        "+\n",
        "50\n",
        "âˆ—\n",
        "65\n",
        "=\n",
        "150\n",
        "+\n",
        "700\n",
        "+\n",
        "1200\n",
        "+\n",
        "2000\n",
        "+\n",
        "3250\n",
        "=\n",
        "7300\n",
        "âˆ‘XY=10âˆ—15+20âˆ—35+30âˆ—40+40âˆ—50+50âˆ—65=150+700+1200+2000+3250=7300\n",
        "\n",
        "âˆ‘\n",
        "ð‘‹\n",
        "2\n",
        "=\n",
        "10\n",
        "2\n",
        "+\n",
        "20\n",
        "2\n",
        "+\n",
        "30\n",
        "2\n",
        "+\n",
        "40\n",
        "2\n",
        "+\n",
        "50\n",
        "2\n",
        "=\n",
        "100\n",
        "+\n",
        "400\n",
        "+\n",
        "900\n",
        "+\n",
        "1600\n",
        "+\n",
        "2500\n",
        "=\n",
        "5500\n",
        "âˆ‘X\n",
        "2\n",
        "=10\n",
        "2\n",
        "+20\n",
        "2\n",
        "+30\n",
        "2\n",
        "+40\n",
        "2\n",
        "+50\n",
        "2\n",
        "=100+400+900+1600+2500=5500\n",
        "\n",
        "Now slope:\n",
        "\n",
        "ð‘Ž\n",
        "=\n",
        "5\n",
        "âˆ—\n",
        "7300\n",
        "âˆ’\n",
        "150\n",
        "âˆ—\n",
        "205\n",
        "5\n",
        "âˆ—\n",
        "5500\n",
        "âˆ’\n",
        "150\n",
        "2\n",
        "=\n",
        "36500\n",
        "âˆ’\n",
        "30750\n",
        "27500\n",
        "âˆ’\n",
        "22500\n",
        "=\n",
        "5750\n",
        "5000\n",
        "=\n",
        "1.15\n",
        "a=\n",
        "5âˆ—5500âˆ’150\n",
        "2\n",
        "5âˆ—7300âˆ’150âˆ—205\n",
        "\tâ€‹\n",
        "\n",
        "=\n",
        "27500âˆ’22500\n",
        "36500âˆ’30750\n",
        "\tâ€‹\n",
        "\n",
        "=\n",
        "5000\n",
        "5750\n",
        "\tâ€‹\n",
        "\n",
        "=1.15\n",
        "\n",
        "Intercept:\n",
        "\n",
        "ð‘\n",
        "=\n",
        "205\n",
        "âˆ’\n",
        "1.15\n",
        "âˆ—\n",
        "150\n",
        "5\n",
        "=\n",
        "205\n",
        "âˆ’\n",
        "172.5\n",
        "5\n",
        "=\n",
        "32.5\n",
        "5\n",
        "=\n",
        "6.5\n",
        "b=\n",
        "5\n",
        "205âˆ’1.15âˆ—150\n",
        "\tâ€‹\n",
        "\n",
        "=\n",
        "5\n",
        "205âˆ’172.5\n",
        "\tâ€‹\n",
        "\n",
        "=\n",
        "5\n",
        "32.5\n",
        "\tâ€‹\n",
        "\n",
        "=6.5\n",
        "\n",
        "âœ… Regression line:\n",
        "\n",
        "ð‘Œ\n",
        "^\n",
        "=\n",
        "1.15\n",
        "ð‘‹\n",
        "+\n",
        "6.5\n",
        "Y\n",
        "^\n",
        "=1.15X+6.5\n",
        "Step 2: Compute Residuals\n",
        "\n",
        "Residuals = Actual\n",
        "ð‘Œ\n",
        "Y âˆ’ Predicted\n",
        "ð‘Œ\n",
        "^\n",
        "Y\n",
        "^\n",
        "\n",
        "X\tY\tÅ¶ = 1.15X+6.5\tResidual = Y-Å¶\n",
        "10\t15\t18.0\t-3.0\n",
        "20\t35\t29.5\t5.5\n",
        "30\t40\t40.0\t0.0\n",
        "40\t50\t51.0\t-1.0\n",
        "50\t65\t62.5\t2.5\n",
        "Step 3: Plot Residuals\n",
        "\n",
        "We plot Residuals vs Predicted Y:\n",
        "\n",
        "X-axis: Predicted\n",
        "ð‘Œ\n",
        "^\n",
        "Y\n",
        "^\n",
        "\n",
        "Y-axis: Residuals\n",
        "\n",
        "We can use Python (Matplotlib) for this:\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = [10, 20, 30, 40, 50]\n",
        "Y = [15, 35, 40, 50, 65]\n",
        "\n",
        "\n",
        "\n",
        "   10. Imagine you are a data scientist working for a real estate company. You\n",
        "need to predict house prices using features like area, number of rooms, and location.\n",
        "However, you detect heteroscedasticity and multicollinearity in your regression\n",
        "model. Explain the steps you would take to address these issues and ensure a robust\n",
        "model.\n",
        "\n",
        ". dealing with two common regression issues: heteroscedasticity and multicollinearity. Here's how you can systematically address them to build a robust house price prediction model.\n",
        "\n",
        "1. Detect and Understand the Issues\n",
        "\n",
        "Heteroscedasticity\n",
        "\n",
        "Occurs when the variance of residuals is not constant across predicted values.\n",
        "\n",
        "Signs: Residuals fan out or contract in a plot of residuals vs. predicted values.\n",
        "\n",
        "Multicollinearity\n",
        "\n",
        "Happens when two or more independent variables are highly correlated.\n",
        "\n",
        "Consequence: Regression coefficients become unstable and interpretation is difficult.\n",
        "\n",
        "Detection:\n",
        "\n",
        "Correlation matrix\n",
        "\n",
        "Variance Inflation Factor (VIF) (>5â€“10 indicates high multicollinearity)\n",
        "\n",
        "2. Steps to Handle Heteroscedasticity\n",
        "\n",
        "Visual inspection\n",
        "\n",
        "Plot residuals vs predicted values to see if variance changes.\n",
        "\n",
        "Transform the dependent variable (Y)\n",
        "\n",
        "Common transforms: log, square root, Box-Cox.\n",
        "\n",
        "Example: log(price) often stabilizes variance for house prices.\n",
        "\n",
        "Weighted Least Squares (WLS)\n",
        "\n",
        "Assign lower weights to observations with higher variance.\n",
        "\n",
        "Useful if you know or can estimate the variance structure.\n",
        "\n",
        "Robust standard errors\n",
        "\n",
        "Use heteroscedasticity-robust standard errors (e.g., Whiteâ€™s correction) for valid inference.\n",
        "\n",
        "3. Steps to Handle Multicollinearity\n",
        "\n",
        "Identify highly correlated variables\n",
        "\n",
        "Use correlation heatmaps or VIF scores.\n",
        "\n",
        "Remove or combine variables\n",
        "\n",
        "Drop redundant features (e.g., if total_rooms and bedrooms are highly correlated, keep only one).\n",
        "\n",
        "Combine variables into meaningful ratios or indexes (e.g., price per square foot).\n",
        "\n",
        "Regularization techniques\n",
        "\n",
        "Ridge regression: Penalizes large coefficients, reduces multicollinearity effects.\n",
        "\n",
        "Lasso regression: Can also select important variables automatically.\n",
        "\n",
        "Principal Component Analysis (PCA)\n",
        "\n",
        "Transform correlated variables into uncorrelated components.\n",
        "\n",
        "Useful if you have many correlated features (like various size-related features).\n",
        "\n",
        "4. Build a Robust Model\n",
        "\n",
        "Start simple\n",
        "\n",
        "Fit a linear regression with transformed target if needed.\n",
        "\n",
        "Check residuals\n",
        "\n",
        "Ensure variance is roughly constant (heteroscedasticity addressed).\n",
        "\n",
        "Check VIF / correlations\n",
        "\n",
        "Ensure multicollinearity is reduced.\n",
        "\n",
        "Validate model\n",
        "\n",
        "Use cross-validation to check model performance.\n",
        "\n",
        "Consider advanced models\n",
        "\n",
        "Tree-based models (Random Forest, XGBoost) are less sensitive to multicollinearity and heteroscedasticity, making them robust alternatives for prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "B17gcL0UT94c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Qo6cjzoVTLex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tCB9n72K1GCs"
      }
    }
  ]
}